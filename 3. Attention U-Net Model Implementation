def attention_block(x, g, filters):
    x_theta = layers.Conv2D(filters, kernel_size=2, strides=(2, 2))(x)
    g_phi = layers.Conv2D(filters, kernel_size=1)(g)
    add = layers.Add()([x_theta, g_phi])
    relu = layers.Activation('relu')(add)
    psi = layers.Conv2D(1, kernel_size=1, activation='sigmoid')(relu)
    upsample_psi = layers.UpSampling2D(size=(2, 2))(psi)
    out = layers.Multiply()([x, upsample_psi])
    return out

def attention_unet(input_shape):
    inputs = layers.Input(input_shape)
    
    # Encoder
    conv1 = conv_block(inputs, 64)
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)
    
    conv2 = conv_block(pool1, 128)
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)
    
    conv3 = conv_block(pool2, 256)
    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)
    
    # Bottleneck
    conv4 = conv_block(pool3, 512)
    
    # Decoder with attention blocks
    attn1 = attention_block(conv3, conv4, 256)
    up1 = layers.UpSampling2D(size=(2, 2))(conv4)
    merge1 = layers.concatenate([attn1, up1], axis=3)
    conv5 = conv_block(merge1, 256)
    
    attn2 = attention_block(conv2, conv5, 128)
    up2 = layers.UpSampling2D(size=(2, 2))(conv5)
    merge2 = layers.concatenate([attn2, up2], axis=3)
    conv6 = conv_block(merge2, 128)
    
    attn3 = attention_block(conv1, conv6, 64)
    up3 = layers.UpSampling2D(size=(2, 2))(conv6)
    merge3 = layers.concatenate([attn3, up3], axis=3)
    conv7 = conv_block(merge3, 64)
    
    outputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(conv7)
    
    model = Model(inputs, outputs)
    return model
